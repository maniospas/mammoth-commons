<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Analysis metrics</title>
    <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">
    <link rel="stylesheet" href="{{ url_for('static', filename='styles.css') }}">
    <style>
        .main-content {
            margin-top: 60px;
        }

        .side-menu {
            height: 100vh;
            position: fixed;
            top: 0;
            left: 0;
            width: 200px;
            padding-top: 70px;
            background-color: #f8f9fa;
            border-right: 1px solid #dee2e6;
        }

        .side-menu a {
            display: block;
            padding: 5px 20px;
            color: #495057;
            text-decoration: none;
        }

        .side-menu a:hover {
            background-color: #e9ecef;
        }

        .content-wrapper {
            margin-left: 220px;
            margin-right: 220px;
            padding: 20px;
        }

        h2 {
            color: #007bff;
            border-bottom: 3px solid #007bff;
            padding-bottom: 5px;
            margin-top: 20px;
        }

        /* Hide navbar on small screens */
        @media (max-width: 576px) {
            .side-menu {
                display: none;
            }

            .content-wrapper {
                margin-left: 0;
                margin-right: 0;
                padding: 0;
            }

            .main-content {
                margin-top: 200px;
            }
        }
    </style>
</head>

<body>
    <!-- Top Navbar -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top">
        <div class="mx-auto">
            <ul class="navbar-nav">
                <li class="nav-item">
                    <a class="nav-link" href="index.html">Home</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="datasets.html">Datasets</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="models.html">Models</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="analysis_methods.html">Analysis</a>
                </li>
            </ul>
        </div>
    </nav>

    <!-- Side Menu -->
    <div class="side-menu">
        <a href='#card'>Card</a>
        <a href='#interactive-report'>Interactive report</a>
        <a href='#interactive-sklearn-report'>Interactive sklearn report</a>
        <a href='#image-bias-analysis'>Image bias analysis</a>
        <a href='#facex-regions'>Facex regions</a>
        <a href='#facex-embeddings'>Facex embeddings</a>
        <a href='#connection-properties'>Connection properties</a>
        <a href='#exposure-distance-comparison'>Exposure distance comparison</a>

    </div>

    <!-- Main Content -->
    <div class="content-wrapper">
        <div class="main-content container">
            <div class="container">
                <h1 class='display-4 text-center my-4'>Analysis metrics</h1>
                <h2 id='card'>Card</h2>
                <p>
                <p>Creates a model card using the <a href="https://github.com/mever-team/FairBench">FairBench</a>
                    library. The card includes several fairness stamps; these are specific measures of bias or fairness
                    that are commonly used in the algorithmic fairness literature. Only the most prominent of those
                    measures are used as stamps, and they correspond to a perfunctory fairness analysis.</p>
                <p>This module computes all applicable FairBench stamps, which summarize behavior across all population
                    groups or intersectional subgroups. Multiple sensitive attributes may be present, such as gender,
                    age, and race. Furthermore, each of those attributes may obtain multiple values, as happens when
                    multiple genders or races are considered. Numeric attributes, like age, are normalized to the range
                    [0,1] and we consider the result as truth values of membership to the group of the maximum value -
                    as opposed to membership to the group with minimum value. A different stamp is computed for each
                    prediction label.</p>
                <p>You may optionally create intersectional subgroups, that is, create a separate subgroup for each
                    combination of sensitive attribute values. Many of those groups will have few members if there are
                    too many attributes, and empty groups are ignored during the analysis.</p>
                <p>The created model card contains exact descriptions of methods used to compute fairness under the
                    selected stamps, and it lists population groups that were taken into account These come alongside an
                    extensive list of caveats and recommendations that help the reader get a grasp on how they should
                    account for the social context. This material is retrieved from FairBench's online socio-technical
                    database generated through MAMMOth's multidisciplinary activities.</p>
                <p>Finally, the generated model card may contain details about out-of-the-box datasets. To get the full
                    picture, a detailed fairness report that also allows you to backtrack computations is available in
                    the <code>interactive report</code> module.</p>
                </p>
                <b>Parameters</b><br> <button type="button" class="btn btn-light" data-bs-toggle="tooltip"
                    data-bs-placement="top"
                    title=" Whether to consider all non-empty group intersections during analysis. This does nothing if there is only one sensitive attribute, but may also be computationally intensive if too many group intersections are selected."
                    data-description=" Whether to consider all non-empty group intersections during analysis. This does nothing if there is only one sensitive attribute, but may also be computationally intensive if too many group intersections are selected."
                    data-name="Intersectional" onclick="showDescriptionModal(this)">
                    <i class="bi bi-info-circle"></i> Intersectional
                </button>
                <button type="button" class="btn btn-light" data-bs-toggle="tooltip" data-bs-placement="top"
                    title=" Whether to compare groups pairwise, or each group to the whole population. For example, if the 4/5ths rule stamp is applicable, it computes positive rates and obtains the minimum ratio, either across all pairs of groups (for pairwise comparison) or otherwise between each group and the total population."
                    data-description=" Whether to compare groups pairwise, or each group to the whole population. For example, if the 4/5ths rule stamp is applicable, it computes positive rates and obtains the minimum ratio, either across all pairs of groups (for pairwise comparison) or otherwise between each group and the total population."
                    data-name="Compare groups" onclick="showDescriptionModal(this)">
                    <i class="bi bi-info-circle"></i> Compare groups
                </button>
                <br>
                <h2 id='interactive-report'>Interactive report</h2>
                <p>
                <p>Creates an interactive report using the FairBench library. The report creates traceable evaluations
                    that you can shift through to find actual sources of unfairness.</p>
                </p>
                <b>Parameters</b><br> <button type="button" class="btn btn-light" data-bs-toggle="tooltip"
                    data-bs-placement="top"
                    title=" Whether to consider all non-empty group intersections during analysis. This does nothing if there is only one sensitive attribute."
                    data-description=" Whether to consider all non-empty group intersections during analysis. This does nothing if there is only one sensitive attribute."
                    data-name="Intersectional" onclick="showDescriptionModal(this)">
                    <i class="bi bi-info-circle"></i> Intersectional
                </button>
                <button type="button" class="btn btn-light" data-bs-toggle="tooltip" data-bs-placement="top"
                    title=" Whether to compare groups pairwise, or each group to the whole population."
                    data-description=" Whether to compare groups pairwise, or each group to the whole population."
                    data-name="Compare groups" onclick="showDescriptionModal(this)">
                    <i class="bi bi-info-circle"></i> Compare groups
                </button>
                <br>
                <h2 id='interactive-sklearn-report'>Interactive sklearn report</h2>
                <p>
                <p>Creates an interactive report using the FairBench library, after running an internal training-test
                    split on a basic sklearn model. The report creates traceable evaluations that you can shift through
                    to find sources of unfairness on a common task.</p>
                </p>
                <b>Parameters</b><br> <button type="button" class="btn btn-light" data-bs-toggle="tooltip"
                    data-bs-placement="top" title=" Which sklearn predictor should be used."
                    data-description=" Which sklearn predictor should be used." data-name="Predictor"
                    onclick="showDescriptionModal(this)">
                    <i class="bi bi-info-circle"></i> Predictor
                </button>
                <button type="button" class="btn btn-light" data-bs-toggle="tooltip" data-bs-placement="top"
                    title=" Whether to consider all non-empty group intersections during analysis. This does nothing if there is only one sensitive attribute."
                    data-description=" Whether to consider all non-empty group intersections during analysis. This does nothing if there is only one sensitive attribute."
                    data-name="Intersectional" onclick="showDescriptionModal(this)">
                    <i class="bi bi-info-circle"></i> Intersectional
                </button>
                <button type="button" class="btn btn-light" data-bs-toggle="tooltip" data-bs-placement="top"
                    title=" Whether to compare groups pairwise, or each group to the whole population."
                    data-description=" Whether to compare groups pairwise, or each group to the whole population."
                    data-name="Compare groups" onclick="showDescriptionModal(this)">
                    <i class="bi bi-info-circle"></i> Compare groups
                </button>
                <br>
                <h2 id='image-bias-analysis'>Image bias analysis</h2>
                <p>
                <p>This module provides a comprehensive solution for analyzing image bias and recommending effective
                    mitigation strategies. It can be used for both classification tasks (e.g., facial attribute
                    extraction) and face verification. The core functionality revolves around evaluating how well
                    different population groups, defined by a given protected attribute (such as gender, age, or
                    ethnicity), are represented in the dataset. Representation bias occurs when some groups are
                    overrepresented or underrepresented, leading to models that may perform poorly or unfairly on
                    certain groups.</p>

                <p>Additionally, the module detects spurious correlations between the target attribute (e.g., the
                    label a model is trying to predict) and other annotated attributes (such as image features like
                    color or shape). Spurious correlations are misleading patterns that do not reflect meaningful
                    relationships and can cause a model to make biased or inaccurate predictions. By identifying and
                    addressing these hidden biases, the module helps improve the fairness and accuracy of your model.
                </p>

                <p>When you run the analysis, the module identifies specific biases within the dataset and suggests
                    tailored mitigation approaches. Specifically, the suitable mitigation methodologies are determined
                    based on the task and the types of the detected biases in the data. </p>

                <p>The analysis is conducted based on the <a
                        href="https://github.com/gsarridis/cv-bias-mitigation-library">CV Bias Mitigation Library</a>.
                </p>
                </p>
                <b>Parameters</b><br> <button type="button" class="btn btn-light" data-bs-toggle="tooltip"
                    data-bs-placement="top"
                    title=" The type of predictive task. It should be either face verification or image classification."
                    data-description=" The type of predictive task. It should be either face verification or image classification."
                    data-name="Task" onclick="showDescriptionModal(this)">
                    <i class="bi bi-info-circle"></i> Task
                </button>
                <br>
                <h2 id='facex-regions'>Facex for face attribute classifiers</h2>
                <p>
                <p>FaceX is a tool designed to help you understand how face attribute classifiers make decisions. It
                    provides clear explanations by analyzing 19 key regions of the face, such as the eyes, nose, mouth,
                    hair, and skin. This method helps reveal which parts of the face the model focuses on when making
                    predictions about attributes like age, gender, or race.</p>
                <p>Rather than explaining each individual image separately, FaceX aggregates information across the
                    entire dataset, offering a broader view of the model's behavior. It looks at how the model activates
                    different regions of the face for each decision. This aggregated information helps
                    you see which facial features are most influential in the model's predictions, and whether certain
                    features are being emphasized more than others.</p>
                <p>In addition to providing an overall picture of which regions are important, FaceX also zooms in on
                    specific areas of the face - such as a section of the skin or a part of the hair - showing which
                    patches of the image have the highest impact on the model's decision. This makes it easier to
                    identify potential biases or problems in how the model is interpreting the face.</p>
                <p>Overall, with FaceX, you can quickly and easily get a better understanding of your model’s
                    decision-making
                    process. This is especially useful for
                    ensuring that your model is fair and transparent, and for spotting any potential biases that may
                    affect
                    its performance.</p>
                <span class="alert alert-warning alert-dismissible fade show" role="alert"
                    style="display: inline-block; padding: 10px;"> <i class="bi bi-exclamation-triangle-fill"></i> GPU
                    access is recommended as this analysis can be computationally intensive, especially with large
                    datasets. </span> </p>

                <b>Parameters</b><br> <button type="button" class="btn btn-light" data-bs-toggle="tooltip"
                    data-bs-placement="top" title=" The integer identifier of the target class."
                    data-description="This parameter allows you to specify which class you want to analyze. For example, if you are using the model to classify faces by gender, setting the target class to male (i.e., the integer identifier for males class) will show you how the model makes decisions about male faces."
                    data-name="Target class" onclick="showDescriptionModal(this)">
                    <i class="bi bi-info-circle"></i> Target class
                </button>
                <button type="button" class="btn btn-light" data-bs-toggle="tooltip" data-bs-placement="top"
                    title=" The layer to be explained."
                    data-description="This parameter lets you choose which part of the model’s neural network you want to analyze. In simple terms, a model consists of multiple layers that process information at different levels. The target layer refers to the specific layer in the model that you want to explain. The explanation will show you which regions of the face are most important to that layer’s decision-making process. For example, the deeper layers of the model may focus on more complex features like facial structure, while earlier layers might focus on simpler features like edges and textures. Typically, you should opt for the last layer prior to the classification layer."
                    data-name="Target layer" onclick="showDescriptionModal(this)">
                    <i class="bi bi-info-circle"></i> Target layer
                </button>
                <br>
                <h2 id='facex-embeddings'>Facex for feature extractors</h2>
                <p>
                <p>FaceX for feature extractors is designed to help you understand how face verification models process
                    images by comparing feature embeddings. In tasks like face recognition, models
                    generate a feature vector (embedding) for each image. These embeddings capture the unique
                    characteristics of the image and can be compared to determine how similar or different two images
                    are. FaceX helps explain which parts of an image contribute to its similarity or difference to a
                    reference embedding, allowing you to understand the model’s focus on specific facial
                    features during the comparison.</p>
                <p>In this context, the key idea is that FaceX analyzes the facial regions in the image that most
                    influence how the model compares the reference embedding with the new image’s embedding. Rather than
                    providing an explanation for individual images in isolation, FaceX aggregates information across the
                    dataset, offering insights into how different parts of the face, such as the eyes, mouth, or hair,
                    contribute to the similarity or difference between the reference and the image being compared.</p>
                <p>FaceX works by using a "reference" image (e.g., identity image) and evaluating how other images
                    (e.g., selfies) compare to it.
                    It looks at how various facial regions of the new image align with the reference image in the
                    feature space. The tool then highlights which facial regions are most influential in the comparison,
                    showing what aspects of the image are similar to or
                    different from the concept embedding.</p>
                <p>Overall, FaceX gives you a better understanding of how the model processes and compares facial
                    features by highlighting the specific regions that influence the similarity or difference in feature
                    embeddings. This is especially useful for improving transparency and identifying potential biases in
                    how face verification models represent and compare faces.</p>
                <span class="alert alert-warning alert-dismissible fade show" role="alert"
                    style="display: inline-block; padding: 10px;"> <i class="bi bi-exclamation-triangle-fill"></i> GPU
                    access is recommended as this analysis can be computationally intensive, especially with large
                    datasets. </span> </p>
                <b>Parameters</b><br> <button type="button" class="btn btn-light" data-bs-toggle="tooltip"
                    data-bs-placement="top" title=" The integer identifier of the target class."
                    data-description="This variable determines what kind of comparison you want to explore. If you set the target class to 1, FaceX will investigate the regions in the image that are similar to the reference embedding (i.e., explanations on why the model consider the two images similar). If you set the target class to 0, FaceX will show the regions that are most different from the reference embedding (i.e., explanations on why the model consider the two images dissimilar)."
                    data-name="Target class" onclick="showDescriptionModal(this)">
                    <i class="bi bi-info-circle"></i> Target class
                </button>
                <button type="button" class="btn btn-light" data-bs-toggle="tooltip" data-bs-placement="top"
                    title=" The layer to be explained."
                    data-description="This parameter lets you choose which part of the model’s neural network you want to analyze. In simple terms, a model consists of multiple layers that process information at different levels. The target layer refers to the specific layer in the model that you want to explain. The explanation will show you which regions of the face are most important to that layer’s decision-making process. For example, the deeper layers of the model may focus on more complex features like facial structure, while earlier layers might focus on simpler features like edges and textures. Typically, you should opt for the last layer producing the final embeddings."
                    data-name="Target layer" onclick="showDescriptionModal(this)">
                    <i class="bi bi-info-circle"></i> Target layer
                </button>
                <br>
                <h2 id='connection-properties'>Connection properties</h2>
                <p>
                <p>Performs analysis of connection properties in a graph. If no sensitive attributes are provided, all
                    node column attributes are considered sensitive.</p>
                </p>
                <h2 id='exposure-distance-comparison'>Exposure distance comparison</h2>
                <p>
                <p>Compute the exposure distance between the protected and non-protected groups in the dataset and
                    ranking. Parameters:</p>
                <ul>
                    <li>
                        <p><code>N runs</code>: Choose a natural number between 1 and 100</p>
                    </li>
                    <li>
                        <p><code>Sensitive attributes</code>: Which attribute is relevant for fairness analysis. To
                            select this, click the blue '+' and then use the dropdown. Currently, only <em>Gender</em>
                            is supported</p>
                    </li>
                    <li>
                        <p><code>Protected</code>: The protected group for the fairness analysis. Currenly, only
                            <em>female</em> or <em>male</em> are supported
                        </p>
                    </li>
                    <li>
                        <p><code>Sampling Attribute</code>: The value by which we group the analysis for finer-grained
                            results. One of <em>Nationality&#95;IncomeGroup</em> or <em>Nationality&#95;Region</em>.</p>
                    </li>
                    <li>
                        <p><code>Ranking Variable</code>: This refers to the main criteria by which ranking is done. One
                            of <em>Degree</em>, <em>Citations</em> or <em>Productivity</em></p>
                    </li>
                </ul>
                </p>
                <b>Parameters</b><br>N runs
                Protected
                Sampling attribute
                Ranking variable
                <br>

            </div>
        </div>
    </div>

    <!-- Description Modal -->
    <div class="modal fade" id="descriptionModal" tabindex="-1" aria-labelledby="descriptionModalLabel"
        aria-hidden="true">
        <div class="modal-dialog modal-lg modal-dialog-centered">
            <div class="modal-content">
                <div class="modal-header">
                    <h5 class="modal-title text-muted" id="descriptionModalLabel">Description</h5>
                    <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
                </div>
                <div class="modal-body" id="descriptionModalBody">
                    <!-- Description will be injected here -->
                </div>
            </div>
        </div>
    </div>

    <script>
        function showDescriptionModal(button) {
            const description = button.getAttribute("data-description");
            const name = button.getAttribute("data-name");
            document.getElementById("descriptionModalLabel").innerText = name;
            document.getElementById("descriptionModalBody").innerText = description;
            const modal = new bootstrap.Modal(document.getElementById("descriptionModal"));
            modal.show();
        }
    </script>
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
</body>

</html>